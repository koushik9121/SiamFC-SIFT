{"cells":[{"cell_type":"markdown","metadata":{"id":"OZyBuMSqe542"},"source":["# CSE527 Homework4 - Part 2 (60 points)\n","**Due date: 23:59 on Dec 17, 2023**\n","\n","---\n","In this semester, we will use Google Colab for the assignments, which allows us to utilize resources that some of us might not have in their local machines such as GPUs. You will need to use your Stony Brook (*.stonybrook.edu) account for coding and Google Drive to save your results.\n","\n","## Google Colab Tutorial\n","---\n","Go to https://colab.research.google.com/notebooks/, you will see a tutorial named \"Welcome to Colaboratory\" file, where you can learn the basics of using google colab.\n","\n","Settings used for assignments: ***Edit -> Notebook Settings -> Runtime Type (Python 3)***.\n","\n","\n","## Local Machine Prerequisites\n","---\n","Since we are using Google Colab, all the code is run on the server environment where lots of libraries or packages have already been installed. In case of missing\n"," libraries or if you want to install them in your local machine, below are the links for installation.\n","* **Install Python 3.6**: https://www.python.org/downloads/ or use Anaconda (a Python distribution) at https://docs.continuum.io/anaconda/install/. Below are some materials and tutorials which you may find useful for learning Python if you are new to Python.\n","  - https://docs.python.org/3.6/tutorial/index.html\n","  - https://www.learnpython.org/\n","  - http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_tutorials.html\n","  - http://www.scipy-lectures.org/advanced/image_processing/index.html\n","\n","\n","* **Install Python packages**: install Python packages: `numpy`, `matplotlib`, `opencv-python` using pip, for example:\n","```\n","pip install numpy matplotlib opencv-python\n","```\n","\tNote that when using “pip install”, make sure that the version you are using is python3. Below are some commands to check which python version it uses in you machine. You can pick one to execute:\n","  \n","```  \n","    pip show pip\n","\n","    pip --version\n","\n","    pip -V\n","\n","```\n","\n","Incase of wrong version, use pip3 for python3 explictly.\n","\n","* **Install Jupyter Notebook**: follow the instructions at http://jupyter.org/install.html to install Jupyter Notebook and familiarize yourself  with it. *After you have installed Python and Jupyter Notebook, please open the notebook file 'HW1.ipynb' with your Jupyter Notebook and do your homework there.*\n","\n","## Description\n","---\n","In this homework you will experiment with SIFT features for scene matching and object recognition. You will work with the SIFT tutorial and code from the University of Toronto. In the compressed homework file, you will find the tutorial document (tutSIFT04.pdf) and a paper from the International Journal of Computer Vision (ijcv04.pdf) describing SIFT and object recognition. Although the tutorial document assumes matlab implemention, you should still be able to follow the technical details in it. In addition, you are **STRONGLY** encouraged to read this paper unless you’re already quite familiar with matching and recognition using SIFT.\n","\n","\n","## Using SIFT in OpenCV 3.x.x in Local Machine\n","---\n","Feature descriptors like SIFT and SURF are no longer included in OpenCV since version 3. This section provides instructions on how to use SIFT for those who use OpenCV 3.x.x. If you are using OpenCV 2.x.x then you are all set, please skip this section. Read this if you are curious about why SIFT is removed https://www.pyimagesearch.com/2015/07/16/where-did-sift-and-surf-go-in-opencv-3/.\n","\n","**We strongly recommend you to use SIFT methods in Colab for this homework**, the details will be described in the next section.\n","\n","However, if you want to use SIFT in your local machine, one simple way to use the OpenCV in-built function `SIFT` is to switch back to version 2.x.x, but if you want to keep using OpenCV 3.x.x, do the following:\n","1. uninstall your original OpenCV package\n","2. install opencv-contrib-python using pip (pip is a Python tool for installing packages written in Python), please find detailed instructions at https://pypi.python.org/pypi/opencv-contrib-python\n","\n","After you have your OpenCV set up, you should be able to use `cv2.xfeatures2d.SIFT_create()` to create a SIFT object, whose functions are listed at http://docs.opencv.org/3.0-beta/modules/xfeatures2d/doc/nonfree_features.html\n","\n","## Using SIFT in OpenCV 3.x.x in Colab (RECOMMENDED)\n","---\n","The default version of OpenCV in Colab is 4.8.0 as of Nov 2023. It also has opencv contrib installed and we can use SIFT method directly without any error. However this was not the case previously and students had to install opencv-contrib manually.  \n","\n"," You should be able to use use `cv2.xfeatures2d.SIFT_create()` to create a SIFT object, whose functions are listed at http://docs.opencv.org/3.0-beta/modules/xfeatures2d/doc/nonfree_features.html\n","\n","## Some Resources\n","---\n","In addition to the tutorial document, the following resources can definitely help you in this homework:\n","- http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_matcher/py_matcher.html\n","- http://docs.opencv.org/3.1.0/da/df5/tutorial_py_sift_intro.html\n","- http://docs.opencv.org/3.0-beta/modules/xfeatures2d/doc/nonfree_features.html?highlight=sift#cv2.SIFT\n","- http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_imgproc/py_geometric_transformations/py_geometric_transformations.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"omyazyywtKQS"},"outputs":[],"source":["# Run this if you are using local (Not tested)\n","# pip install the OpenCV version from 'contrib'\n","# opencv-contrib-python==4.8.0.76\n","# opencv-python==4.8.0.76"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":87,"status":"ok","timestamp":1703042252590,"user":{"displayName":"Sai Mupparapu","userId":"14974658478719573045"},"user_tz":300},"id":"wnIVJsktWz5-","outputId":"6862341c-6c13-40f0-fe65-df130d515685"},"outputs":[{"output_type":"stream","name":"stdout","text":["4.8.0\n"]}],"source":["# import packages here\n","import cv2\n","import math\n","import numpy as np\n","import matplotlib.pyplot as plt\n","print(cv2.__version__) # verify OpenCV version\n","import os"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":773,"status":"ok","timestamp":1703042255370,"user":{"displayName":"Sai Mupparapu","userId":"14974658478719573045"},"user_tz":300},"id":"DRJ7cx71t8XR","outputId":"9a4b2f96-139c-4347-e970-9161a86f4fa2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"]}],"source":["# Mount your google drive where you've saved your assignment folder\n","from google.colab import drive\n","drive.mount('/gdrive')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":112,"status":"ok","timestamp":1703042257514,"user":{"displayName":"Sai Mupparapu","userId":"14974658478719573045"},"user_tz":300},"id":"DYCDG18Ht50P","outputId":"99e6d53c-38e4-4c1b-b83c-1ab36c32543f"},"outputs":[{"output_type":"stream","name":"stdout","text":["/gdrive/MyDrive/MUPPARAPU_SAIKOUSHIK_114999629_hw4/part2\n"]}],"source":["# Replace '------' with the path such that \"CSE527-20S-HW2\" is your working directory\n","%cd '/gdrive/MyDrive/MUPPARAPU_SAIKOUSHIK_114999629_hw4/part2'\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":238,"status":"ok","timestamp":1703042259752,"user":{"displayName":"Sai Mupparapu","userId":"14974658478719573045"},"user_tz":300},"id":"DPHHxAMz3bC4","outputId":"9a63def9-f37b-490c-a0c5-96d1ca02521a"},"outputs":[{"output_type":"stream","name":"stdout","text":["CSE527_23F_HW4_P2.ipynb  ijcv04.pdf  SourceImages  stitched.png  tutSIFT04.pdf\n"]}],"source":["!ls"]},{"cell_type":"markdown","metadata":{"id":"mROKmAhve545"},"source":["## Problem 1: Match transformed images using SIFT features\n","{10 points} You will transform a given image, and match it back to the original image using SIFT keypoints.\n","\n","- **Step 1 **. Use the function from SIFT class to detect keypoints from the given image. Plot the image with keypoints scale and orientation overlaid.\n","\n","- **Step 2 **. Rotate your image clockwise by 45 degrees with the `cv2.warpAffine` function. Extract SIFT keypoints for this rotated image and plot the rotated picture with keypoints scale and orientation overlaid just as in step 1.\n","\n","- **Step 3 **. Match the SIFT keypoints of the original image and the rotated imag using the `knnMatch` function in the `cv2.BFMatcher` class. Discard bad matches using the ratio test proposed by D.Lowe in the SIFT paper. Use **0.1** as the ratio in this homework. Note that this is for display purpose only. Draw the filtered good keypoint matches on the image and display it. The image you draw should have two images side by side with matching lines across them.\n","\n","- **Step 4 **. Use the RANSAC algorithm to find the affine transformation from the rotated image to the original image. You are not required to implement the RANSAC algorithm yourself, instead you could use the `cv2.findHomography` function (set the 3rd parameter `method` to `cv2.RANSAC`) to compute the transformation matrix. Transform the rotated image back using this matrix and the `cv2.warpPerspective` function. Display the recovered image.\n","\n","-  You might have noticed that the rotated image from step 2 is cropped. Try rotating the image without any cropping.\n","\n","Hints: In case of too many matches in the output image, use the ratio of 0.1 to filter matches.\n","\n","The image is a duplicate of *Table in front of window* by Pablo Picasso. See https://www.pablopicasso.org/ for more stories about Pablo Picasso and https://www.wikiart.org/en/pablo-picasso/table-in-front-of-window-1919 for more information about this work.\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1xUKDDuutGru8IMViY92noGjXzGwWEaEQ"},"executionInfo":{"elapsed":2834,"status":"ok","timestamp":1703042264994,"user":{"displayName":"Sai Mupparapu","userId":"14974658478719573045"},"user_tz":300},"id":"tn4AgU_lLp6_","outputId":"67edb00b-2181-4edc-cd42-b8fdef224629"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["def drawMatches(img1, kp1, img2, kp2, matches):\n","    \"\"\"\n","    My own implementation of cv2.drawMatches as OpenCV 2.4.9\n","    does not have this function available but it's supported in\n","    OpenCV 3.0.0\n","\n","    This function takes in two images with their associated\n","    keypoints, as well as a list of DMatch data structure (matches)\n","    that contains which keypoints matched in which images.\n","\n","    An image will be produced where a montage is shown with\n","    the first image followed by the second image beside it.\n","\n","    Keypoints are delineated with circles, while lines are connected\n","    between matching keypoints.\n","\n","    img1,img2 - Grayscale or Color images\n","    kp1,kp2 - Detected list of keypoints through any of the OpenCV keypoint\n","              detection algorithms\n","    matches - A list of matches of corresponding keypoints through any\n","              OpenCV keypoint matching algorithm\n","    \"\"\"\n","\n","    # Create a new output image that concatenates the two images together\n","    # (a.k.a) a montage\n","    rows1 = img1.shape[0]\n","    cols1 = img1.shape[1]\n","    rows2 = img2.shape[0]\n","    cols2 = img2.shape[1]\n","\n","    # Create the output image\n","    # The rows of the output are the largest between the two images\n","    # and the columns are simply the sum of the two together\n","    # The intent is to make this a colour image, so make this 3 channels\n","    out = np.zeros((max([rows1,rows2]),cols1+cols2,3), dtype='uint8')\n","\n","    # Place the first image to the left\n","    # stack if the inputs are gray images\n","    if len(img1.shape) == 2:\n","      img1 = np.dstack([img1, img1, img1])\n","    if len(img2.shape) == 2:\n","      img2 = np.dstack([img2, img2, img2])\n","\n","    out[:rows1,:cols1, :] = img1\n","\n","    # Place the next image to the right of it\n","    out[:rows2,cols1:, :] = img2\n","\n","    # For each pair of points we have between both images\n","    # draw circles, then connect a line between them\n","    for mat in matches:\n","        # Get the matching keypoints for each of the images\n","        img1_idx = mat.queryIdx\n","        img2_idx = mat.trainIdx\n","\n","        # x - columns\n","        # y - rows\n","        (x1,y1) = kp1[img1_idx].pt\n","        (x2,y2) = kp2[img2_idx].pt\n","\n","        # Draw a small circle at both co-ordinates\n","        # radius 4\n","        # colour blue\n","        # thickness = 1\n","        cv2.circle(out, (int(x1),int(y1)), 4, (255, 0, 0), 1)\n","        cv2.circle(out, (int(x2)+cols1,int(y2)), 4, (255, 0, 0), 1)\n","\n","        # Draw a line in between the two points\n","        # thickness = 1\n","        # colour blue\n","        cv2.line(out, (int(x1),int(y1)), (int(x2)+cols1,int(y2)), (0,255,0), 2)\n","    # Also return the image if you'd like a copy\n","    return out\n","\n","# Read image\n","img_input = cv2.imread('SourceImages/Picasso.png')\n","\n","##########--WRITE YOUR CODE HERE--##########\n","\n","sift_detector_original = cv2.SIFT_create()\n","keypoints_orig, descriptors_orig = sift_detector_original.detectAndCompute(img_input, None)\n","res1= cv2.drawKeypoints(img_input, keypoints_orig, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n","height_orig, width_orig = img_input.shape[:2]\n","val=np.pi / 4\n","cos_theta=np.cos(val)\n","sin_theta =np.sin(val)\n","a=width_orig * cos_theta\n","b=height_orig * sin_theta\n","c=width_orig * sin_theta\n","d=height_orig * cos_theta\n","updated_width=int(a+b)\n","updated_height=int(c+d)\n","a_1=width_orig // 2\n","a_2=height_orig // 2\n","rotation_matrix_adjusted = cv2.getRotationMatrix2D((a_1,a_2), 315, 1)\n","val1=updated_width - width_orig\n","val2=updated_height - height_orig\n","rotation_matrix_adjusted[0, 2] = rotation_matrix_adjusted[0, 2]+(val1) // 2\n","rotation_matrix_adjusted[1, 2] = rotation_matrix_adjusted[1, 2]+(val2) // 2\n","rotated_img_input = cv2.warpAffine(img_input, rotation_matrix_adjusted, (updated_width, updated_height))\n","sift_detector_rotated = cv2.SIFT_create()\n","keypoints_rot, descriptors_rot = sift_detector_rotated.detectAndCompute(rotated_img_input, None)\n","res2= cv2.drawKeypoints(rotated_img_input, keypoints_rot, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n","\n","\n","\n","##########-------END OF CODE-------##########\n","\n","# Plot result images\n","plt.figure(figsize=(14,8))\n","plt.subplot(1, 2, 1)\n","plt.imshow(cv2.cvtColor(res1, cv2.COLOR_BGR2RGB));\n","plt.title('original img')\n","plt.axis('off')\n","\n","plt.subplot(1, 2, 2)\n","plt.imshow(cv2.cvtColor(res2, cv2.COLOR_BGR2RGB));\n","plt.title('rotated img')\n","plt.axis('off')\n","\n","##########--WRITE YOUR CODE HERE--##########\n","\n","bf_matcher = cv2.BFMatcher()\n","knn_matches = bf_matcher.knnMatch(descriptors_orig, descriptors_rot, k=2)\n","filtered_matches = [m for m, n in knn_matches if m.distance < 0.1 * n.distance]\n","res3 = drawMatches(img_input, keypoints_orig, rotated_img_input, keypoints_rot, filtered_matches)\n","\n","##########-------END OF CODE-------##########\n","\n","plt.figure(figsize=(14,8))\n","plt.imshow(cv2.cvtColor(res3, cv2.COLOR_BGR2RGB));\n","plt.title('matching')\n","plt.axis('off')\n","\n","##########--WRITE YOUR CODE HERE--##########\n","\n","\n","\n","d_pts = np.float32([keypoints_orig[m.queryIdx].pt for m in filtered_matches]).reshape(-1, 1, 2)\n","s_pts = np.float32([keypoints_rot[m.trainIdx].pt for m in filtered_matches]).reshape(-1, 1, 2)\n","homography_matrix, _ = cv2.findHomography(s_pts, d_pts, cv2.RANSAC)\n","res4 = cv2.warpPerspective(rotated_img_input, homography_matrix, (width_orig, height_orig))\n","\n","\n","##########-------END OF CODE-------##########\n","\n","\n","# plot result images\n","plt.figure(figsize=(14,8));\n","plt.subplot(1, 2, 1);\n","plt.imshow(cv2.cvtColor(img_input, cv2.COLOR_BGR2RGB));\n","plt.title('original img');\n","plt.axis('off');\n","\n","plt.subplot(1, 2, 2);\n","plt.imshow(cv2.cvtColor(res4, cv2.COLOR_BGR2RGB));\n","plt.title('recovered img');\n","plt.axis('off');\n"]},{"cell_type":"markdown","metadata":{"id":"2-hGWnFUe548"},"source":["## Problem 2: Scene stitching with SIFT features\n","{20 points} You will match and align between different views of a scene with SIFT features.\n","\n","Use `cv2.copyMakeBorder` function to pad the center image with zeros into a larger size. Extract SIFT features for all images and go through the same procedures as you did in problem 1. Your goal is to find the affine transformation between the two images and then align one of your images to the other using `cv2.warpPerspective`. Use the `cv2.addWeighted` function (or your own implementation) to blend the aligned images and show the stitched result. Examples can be found at http://docs.opencv.org/trunk/d0/d86/tutorial_py_image_arithmetics.html.\n","Use parameters **0.5 and 0.5** for alpha blending.\n","\n","- **Step 1 (10points) **. Compute the transformation from the right image to the center image. Warp the right image with the computed transformation. Stitch the center and right images with alpha blending. Display the SIFT feature matching between the center and right images like you did in problem 1. Display the stitched result (center and right image).\n","\n","- **Step 2 (5 points)** Compute the transformation from the left image to the stitched image from step 1. Warp the left image with the computed transformation. Stich the left and result images from step 1 with alpha blending. Display the SIFT feature matching between the result image from step 1 and the left image like what you did in problem 1. Display the final stitched result (all three images).\n","\n","- **Laplacian (5 points)**. Instead of using `cv2.addWeighted` to do the blending, implement Laplacian Pyramids to blend the two aligned images. Tutorials can be found at http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_pyramids/py_pyramids.html. Display the stitched result (center and right image) and the final stitched result (all three images) with laplacian blending instead of alpha blending.\n","\n","Note that for the resultant stitched image, some might have different intensity in the overlapping and other regions, namely the overlapping region looks brighter or darker than others. To get full credit, the final image should have uniform illumination.\n","\n","Hints: You need to find the warping matrix between images with the same mechanism from problem 1. You will need as many reliable matches as possible to find a good homography so DO NOT use 0.1 here. A suggested value would be 0.75 in this case.\n","\n","When you warp the image with cv2.warpPerspective, an important trick is to pass in the correct parameters so that the warped image has the same size with the padded_center image. Once you have two images with the same size, find the overlapping part and do the blending.\n","\n","The images are the Stony Brook's Student Activities Center from https://www.youvisit.com/tour/panoramas/stonybrook/80175?id=405."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1tOd9I20iXQREbIdjdfNO8oTJJaQ3Cc2x"},"executionInfo":{"elapsed":27954,"status":"ok","timestamp":1703042300119,"user":{"displayName":"Sai Mupparapu","userId":"14974658478719573045"},"user_tz":300},"id":"K2HhJDJELxb9","outputId":"92e380fc-075a-4640-cb9d-ebea206be7b4"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["imgCenter = cv2.imread('SourceImages/sac_center.png', cv2.IMREAD_COLOR)\n","imgRight  = cv2.imread('SourceImages/sac_r.png', cv2.IMREAD_COLOR)\n","imgLeft   = cv2.imread('SourceImages/sac_l.png', cv2.IMREAD_COLOR)\n","\n","# initalize the stitched image as the center image\n","imgCenter = cv2.copyMakeBorder(imgCenter,160,160,400,400,cv2.BORDER_CONSTANT)\n","print(imgLeft.shape)\n","print(imgCenter.shape)\n","print(imgRight.shape)\n","def image_product(i1,i2):\n","  prod=i1*i2\n","  return prod\n","def alpha_blend(img_A, img_B):\n","    ##########--WRITE YOUR CODE HERE--##########\n","\n","    alpha = 0.5\n","    alpha_1=1 - alpha\n","    con1=image_product(alpha , img_A )\n","    con2=image_product(alpha_1 ,img_B)\n","    con=con1+con2\n","    blended = np.where(img_A > 0, con , img_B)\n","    blended = np.where(img_B > 0, blended, img_A)\n","    ##########-------END OF CODE-------##########\n","    return blended.astype(img_A.dtype)\n","\n","def gauss(img,n):\n","   gaussian = img.copy()\n","   pyramid = [gaussian]\n","   for i in range(n):\n","    gaussian = cv2.pyrDown(gaussian)\n","    pyramid.append(np.float32(gaussian))\n","   return pyramid\n","def Laplacian_Blending(img_A, img_B, mask, num_levels=2):\n","    # Implement Laplacian_blending\n","    assert img_A.shape == img_B.shape\n","    assert img_A.shape == mask.shape\n","\n","    ##########--WRITE YOUR CODE HERE--##########\n","    pyramid_A=gauss(img_A,num_levels)\n","    pyramid_B=gauss(img_B,num_levels)\n","    pyramid_mask=gauss(mask,num_levels)\n","    laplacian_pyramid_A = [pyramid_A[num_levels-1]]\n","    laplacian_pyramid_B = [pyramid_B[num_levels-1]]\n","    mask_pyramid = [pyramid_mask[num_levels-1]]\n","\n","    for i in range(num_levels-1, 0, -1):\n","        laplacian_A = np.subtract(pyramid_A[i-1], cv2.pyrUp(pyramid_A[i]))\n","        laplacian_B = np.subtract(pyramid_B[i-1], cv2.pyrUp(pyramid_B[i]))\n","        laplacian_pyramid_A.append(laplacian_A)\n","        laplacian_pyramid_B.append(laplacian_B)\n","        mask_pyramid.append(pyramid_mask[i-1])\n","\n","    laplacian_blend = []\n","    for la, lb, mask_lvl in zip(laplacian_pyramid_A, laplacian_pyramid_B, mask_pyramid):\n","        masked=1.0 - mask_lvl\n","        val1=np.multiply(la, (masked))\n","        val2=np.multiply(lb, mask_lvl)\n","        blended_layer = val1 + val2\n","        laplacian_blend.append(blended_layer)\n","\n","    final_blend = laplacian_blend[0]\n","    for i in range(1, num_levels):\n","        final_blend = cv2.pyrUp(final_blend)\n","        final_blend = final_blend.astype(laplacian_blend[i].dtype)\n","        final_blend = cv2.add(final_blend, laplacian_blend[i])\n","    final_blend = np.clip(final_blend, 0, 255)\n","    final_blend = final_blend.astype('uint8')\n","\n","    ##########-------END OF CODE-------##########\n","    return final_blend\n","def getTransform(img1, img2):\n","    ##########--WRITE YOUR CODE HERE--##########\n","\n","    sift_detector = cv2.SIFT_create()\n","    keypoints1, descriptors1 = sift_detector.detectAndCompute(img1, None)\n","    keypoints2, descriptors2 = sift_detector.detectAndCompute(img2, None)\n","\n","    matcher = cv2.BFMatcher()\n","    matches_knn = matcher.knnMatch(descriptors1, descriptors2, k=2)\n","\n","    quality_matches = []\n","    for match_pair in matches_knn:\n","      val1=match_pair[0].distance\n","      val2=0.75 * match_pair[1].distance\n","      if val2>val1:\n","            quality_matches.append(match_pair[0])\n","\n","    destination_points = np.float32([keypoints2[m.trainIdx].pt for m in quality_matches])\n","    source_points = np.float32([keypoints1[m.queryIdx].pt for m in quality_matches])\n","    img_match = drawMatches(img1, keypoints1, img2, keypoints2, quality_matches)\n","    H, mask = cv2.findHomography(destination_points, source_points, cv2.RANSAC)\n","\n","    ##########-------END OF CODE-------##########\n","    return H, img_match\n","\n","\n","def perspective_warping_alpha_blending(imgCenter, imgLeft, imgRight):\n","    ##########--WRITE YOUR CODE HERE--##########\n","\n","    transform_CR, img_match_cr = getTransform(imgCenter, imgRight)\n","    CR=(imgCenter.shape[1], imgCenter.shape[0])\n","    right_transformed = cv2.warpPerspective(imgRight, transform_CR,CR )\n","    stitched_cr = alpha_blend(imgCenter, right_transformed)\n","    transform_LCR, img_match_lcr = getTransform(stitched_cr, imgLeft)\n","    LR=(stitched_cr.shape[1], stitched_cr.shape[0])\n","    left_transformed = cv2.warpPerspective(imgLeft, transform_LCR, LR)\n","    stitched_lcr = alpha_blend(left_transformed, stitched_cr)\n","\n","    ##########-------END OF CODE-------##########\n","    return img_match_cr, stitched_cr, img_match_lcr, stitched_lcr\n","\n","\n","def perspective_warping_laplacian_blending(imgCenter, imgLeft, imgRight):\n","    ##########--WRITE YOUR CODE HERE--##########\n","\n","    homography_CR, img_match_cr = getTransform(imgCenter, imgRight)\n","    ri_w=(imgCenter.shape[1], imgCenter.shape[0])\n","    right_img_warped = cv2.warpPerspective(imgRight, homography_CR,ri_w )\n","    ma_r=(imgCenter.shape[1], imgCenter.shape[0])\n","    mask_warped_right = cv2.warpPerspective(np.ones(imgRight.shape, dtype=np.float32), homography_CR,ma_r )\n","    stitched_cr = Laplacian_Blending(imgCenter, right_img_warped, mask_warped_right)\n","    homography_LCR, img_match_lcr = getTransform(np.uint8(stitched_cr), imgLeft)\n","    li_w=(stitched_cr.shape[1], stitched_cr.shape[0])\n","    left_img_warped = cv2.warpPerspective(imgLeft, homography_LCR,li_w )\n","    mask_warped_left = cv2.warpPerspective(np.ones(imgLeft.shape, dtype=np.float32), homography_LCR, (stitched_cr.shape[1], stitched_cr.shape[0]))\n","    stitched_lcr = Laplacian_Blending(stitched_cr, left_img_warped, mask_warped_left)\n","    stitched_cr_ty=np.clip(stitched_cr, 0, 255)\n","    stitched_lcr_ty=np.clip(stitched_lcr, 0, 255)\n","    stitched_cr = stitched_cr_ty.astype(np.uint8)\n","    stitched_lcr = stitched_lcr_ty.astype(np.uint8)\n","\n","    ##########-------END OF CODE-------##########\n","    return img_match_cr, stitched_cr, img_match_lcr, stitched_lcr\n","\n","\n","\n","img_match_cr, stitched_cr, img_match_lcr, stitched_lcr = perspective_warping_alpha_blending(imgCenter, imgLeft, imgRight)\n","img_match_cr_lap, stitched_cr_lap, img_match_lcr_lap, stitched_lcr_lap = perspective_warping_laplacian_blending(imgCenter, imgLeft, imgRight)\n","\n","plt.figure(figsize=(15,30));\n","plt.subplot(4, 1, 1);\n","plt.imshow(cv2.cvtColor(img_match_cr, cv2.COLOR_BGR2RGB));\n","plt.title(\"center and right matches\");\n","plt.axis('off');\n","plt.subplot(4, 1, 2);\n","plt.imshow(cv2.cvtColor(stitched_cr, cv2.COLOR_BGR2RGB));\n","plt.title(\"center, right: stitched result\");\n","plt.axis('off');\n","plt.subplot(4, 1, 3);\n","plt.imshow(cv2.cvtColor(img_match_lcr, cv2.COLOR_BGR2RGB));\n","plt.title(\"left and center_right matches\");\n","plt.axis('off');\n","plt.subplot(4, 1, 4);\n","plt.imshow(cv2.cvtColor(stitched_lcr, cv2.COLOR_BGR2RGB));\n","plt.title(\"left, center, right: stitched result\");\n","plt.axis('off');\n","plt.show();\n","\n","plt.figure(figsize=(15,30));\n","plt.subplot(4, 1, 1);\n","plt.imshow(cv2.cvtColor(stitched_cr_lap, cv2.COLOR_BGR2RGB));\n","plt.title(\"Laplacian - center, right: stitched result\");\n","plt.axis('off');\n","plt.subplot(4, 1, 2);\n","plt.imshow(cv2.cvtColor(stitched_lcr_lap, cv2.COLOR_BGR2RGB));\n","plt.title(\"Laplacian - left, center, right: stitched result\");\n","plt.axis('off');\n"]},{"cell_type":"markdown","metadata":{"id":"gpJS18x1l4Ro"},"source":["#Stitching a set of images\n","\n","This is similar to above problem except that you will be stitching a collection of images, without any given order.\n","You will be given the numpy images in the variable ```image_collection``` and you will have to write your algorithm to stitch and display an image that should resemble below image:\n"," ![Example Image](https://drive.google.com/uc?id=1hF6uT-NmWAapnAKxJqKf644Lb6njFh-2)\n","\n","\n","Please save the output image as  `stitched.png`\n","\n"," For simplicity of this HW, you will be provided the center_image in ```center_image``` and all others will be at ```image_collection```. The final output will be constructed up on the base image which uses ```center_image``` padded with 1600 pixels each on the top and the bottom edges and 3200 pixels each on the left and the right edges.\n","\n","\n","Note:\n","1. You cannot use image stitching libraries availble on the internet. You will have to implement it on your own based on the methods you have alrady implemented above.\n","2. Try not to manually hardcode the order of images for stitching. (10pts)\n","3. If you plan to use any graph algorithm, you may use code from internet,but you must cite the URL/library.\n","4. Please **include comments** where ever possible describing your algorithm.\n","5. Clean your code before submission to be read by TA. Try not to use too many code blocks, it makes your code less readable.\n","6. You can use alpha blending for this problem\n","7. You may experiment on less resolution version of these images to save time. But your submission should be on original resolution images\n","8. If you come across an artifact caused by cv2's warping method, this graphing (https://www.desmos.com/calculator/cefcmi6pvn  made by Kalyan), may help you.\n"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":36893,"status":"ok","timestamp":1703042558915,"user":{"displayName":"Sai Mupparapu","userId":"14974658478719573045"},"user_tz":300},"id":"ssNssgTBKnmK","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b7ea10c1-ebbf-47c6-e29e-ed4e7bc0272c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":7}],"source":["import os\n","from tqdm import tqdm\n","import random\n","import cv2\n","def load_sac_images():\n","    # DO NOT CHANGE THE ORDER IN THIS LIST\n","    filelist=['sac_bus.png', 'sac_rb.png', 'sac_libside.png', 'sac_rsky.png', 'sac_r.png', 'sac_sky.png', 'sac_cb.png', 'sac_l.png' ]\n","    return [cv2.imread(f'SourceImages/{f}') for f in filelist]\n","\n","center_image = cv2.imread(f'SourceImages/sac_center.png')\n","image_collection = load_sac_images()\n","def compute_features(image):\n","    sift_detector = cv2.xfeatures2d.SIFT_create()\n","    key_points, descri = sift_detector.detectAndCompute(image, None)\n","    return key_points, descri\n","\n","def match_features(descriptor1, descriptor2):\n","    bf_matcher = cv2.BFMatcher()\n","    all_matches = bf_matcher.knnMatch(descriptor1, descriptor2, k=2)\n","    good_matches_list = []\n","    for first, second in all_matches:\n","      sd=0.75 * second.distance\n","      if sd>first.distance:\n","        good_matches_list.append(first)\n","    return good_matches_list\n","\n","def matches_best(center_image, image_collection):\n","    center_keypoints, center_descriptors = compute_features(center_image)\n","    matching_info = []\n","    for image in image_collection:\n","        keypoints, descriptors = compute_features(image)\n","        quality_matches = match_features(center_descriptors, descriptors)\n","        count_matches = len(quality_matches)\n","        image_match_data = [image, keypoints, quality_matches, count_matches]\n","        matching_info.append(image_match_data)\n","\n","    matching_info.sort(key=lambda item: item[3], reverse=True)\n","    return matching_info\n","\n","\n","def flip_mask(H, img, base_img):\n","    corners = np.array([[[0, 0], [0, img.shape[0]-1], [img.shape[1]-1, img.shape[0]-1], [img.shape[1]-1, 0]]], dtype=np.float32)\n","    transformed_corners = cv2.perspectiveTransform(corners, H)\n","\n","    upper_right_corner = transformed_corners[0, 2, :]\n","    lower_right_corner = transformed_corners[0, 3, :]\n","    is_left_flipped = upper_right_corner[1] < lower_right_corner[1]\n","    upper_left_corner = transformed_corners[0, 1, :]\n","    lower_left_corner = transformed_corners[0, 0, :]\n","    is_right_flipped = upper_left_corner[1] < lower_left_corner[1]\n","    mask = np.ones_like(img, dtype=np.uint8)\n","    mask = cv2.warpPerspective(mask, H, (base_img.shape[1], base_img.shape[0]))\n","    if is_right_flipped:\n","        mask[:, mask.shape[1]//2:, :] = 0\n","    if is_left_flipped:\n","        mask[:, :mask.shape[1]//2, :] = 0\n","\n","    return mask\n","\n","def stitchedimages(base_img, image_list):\n","    current_stitched_img = base_img\n","    for current_img in image_list:\n","        current_stitched=current_stitched_img.astype('uint8')\n","        current_img_1=current_img.astype('uint8')\n","        transform_matrix, _ = getTransform(current_stitched,current_img_1 )\n","        warped_image = cv2.warpPerspective(current_img, transform_matrix, (current_stitched_img.shape[1], current_stitched_img.shape[0]))\n","        blend_mask = flip_mask(transform_matrix, current_img, current_stitched_img)\n","        masked_warped_image = warped_image * blend_mask\n","        current_stitched_img = alpha_blend(current_stitched_img, masked_warped_image)\n","\n","    return current_stitched_img\n","\n","\n","image_padded = cv2.copyMakeBorder(center_image, 1600, 1600, 3200, 3200, cv2.BORDER_CONSTANT)\n","match_info = matches_best(center_image, image_collection)\n","s_images = [info[0] for info in match_info]\n","stitched = stitchedimages(image_padded, s_images)\n","cv2.imwrite('stitched.png', stitched)\n"]},{"cell_type":"markdown","metadata":{"id":"YhVGbWfXq1_D"},"source":["#################YOUR CODE STARTS HERE #####################\n"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"U_FmkJhWe55I"},"source":["## Submission guidelines\n","---\n","\n","Plagiarism: plagiarism is strictly forbidden.   \n","Note: Please be advised that uploading your homework assignments to public platforms, such as GitHub, is STRICTLY PROHIBITED. Sharing your homework solutions in this manner (even after the course completion) constitutes a violation of academic integrity and will be treated as such.\n","\n"]}],"metadata":{"anaconda-cloud":{},"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":0}